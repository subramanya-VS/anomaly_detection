{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b44695a",
   "metadata": {},
   "source": [
    "# Synthetic Transaction Dataset\n",
    "\n",
    "This notebook generates a realistic synthetic transaction dataset (India context) and injects a variety\n",
    "of fraud patterns (card cloning, account takeover, merchant collusion, velocity probes, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ee3e3",
   "metadata": {},
   "source": [
    "## Final combined pattern list (implemented / referenced)\n",
    "\n",
    "\n",
    "Implemented patterns:\n",
    "- Log-normal amounts by merchant category (realistic skew)\n",
    "- Geospatial centroids for customers and merchants + Haversine distance\n",
    "- Temporal seasonality (hour peaks) and weekly modulation\n",
    "- Sticky customer category preferences\n",
    "- Card cloning: impossible travel & concurrent sessions\n",
    "- Account takeover (ATO): bursty high-value sequences, category shift\n",
    "- Merchant collusion: clique-like behavior and structured amounts near ₹50,000\n",
    "- Velocity probes and small-value testing\n",
    "- Post-generation risk-score adjustment to hit target fraud rate (2%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook configured — will write to ./transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# Standard imports and global configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Output path\n",
    "OUT_DIR = './'\n",
    "OUT_CSV = os.path.join(OUT_DIR, 'transactions.csv')\n",
    "\n",
    "# Dataset size & characteristics\n",
    "N_ROWS = 100_000\n",
    "N_CUSTOMERS = 5000\n",
    "N_MERCHANTS = 500\n",
    "TARGET_FRAUD_RATE = 0.02  \n",
    "\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 3, 31)\n",
    "\n",
    "# Merchant categories & simple regional bounding box (India)\n",
    "MERCHANT_CATEGORIES = ['grocery', 'electronics', 'gas', 'restaurant', 'retail', 'jewelry', 'luxury_goods']\n",
    "LAT_MIN, LAT_MAX = 6.5, 37.8\n",
    "LON_MIN, LON_MAX = 68.0, 97.5\n",
    "\n",
    "print('Notebook configured — will write to', OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "663500dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Return distance in km between two lat/lon points (WGS84 approximation).\"\"\"\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "# Small utility to sample with replacement but reproducibly\n",
    "def rchoice(seq):\n",
    "    return seq[random.randint(0, len(seq)-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "498eed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5000 customers and 500 merchants\n"
     ]
    }
   ],
   "source": [
    "# 1) Create customers and merchants — readable & commented\n",
    "customers = []\n",
    "\n",
    "# Urban centers used as seeds for realistic clustering\n",
    "URBAN_CENTERS = [\n",
    "    (28.7041, 77.1025, 1.0),   # Delhi\n",
    "    (19.0760, 72.8777, 1.0),   # Mumbai\n",
    "    (12.9716, 77.5946, 0.8),   # Bangalore\n",
    "    (13.0827, 80.2707, 0.6),   # Chennai\n",
    "    (22.5726, 88.3639, 0.5),   # Kolkata\n",
    "    (26.9124, 75.7873, 0.3),   # Jaipur\n",
    "]\n",
    "\n",
    "center_weights = np.array([c[2] for c in URBAN_CENTERS], dtype=float)\n",
    "center_weights /= center_weights.sum()\n",
    "\n",
    "for i in range(1, N_CUSTOMERS + 1):\n",
    "    cust_id = f'CUST_{i:05d}'\n",
    "    # pick a city centroid biased by center_weights\n",
    "    idx = np.random.choice(len(URBAN_CENTERS), p=center_weights)\n",
    "    base_lat, base_lon = URBAN_CENTERS[idx][0], URBAN_CENTERS[idx][1]\n",
    "    # jitter the home coordinates a bit — humans live near the city center but not exactly on it\n",
    "    home_lat = base_lat + np.random.normal(scale=0.08)\n",
    "    home_lon = base_lon + np.random.normal(scale=0.08)\n",
    "    # simple spending profile: conservative / normal / high\n",
    "    profile = np.random.choice(['conservative', 'normal', 'high'], p=[0.5, 0.4, 0.1])\n",
    "    base_rate = {'conservative': 0.6, 'normal': 1.0, 'high': 2.0}[profile]\n",
    "    # sticky category preferences (per-customer probability vector)\n",
    "    cat_probs = np.random.dirichlet(np.ones(len(MERCHANT_CATEGORIES)))\n",
    "    customers.append((cust_id, home_lat, home_lon, profile, base_rate, cat_probs.tolist()))\n",
    "\n",
    "cust_df = pd.DataFrame(customers, columns=['customer_id', 'home_lat', 'home_long', 'profile', 'base_rate', 'cat_probs'])\n",
    "\n",
    "# 2) Create merchants with realistic location jitter per category\n",
    "merchants = []\n",
    "category_base_probs = [0.25, 0.12, 0.12, 0.18, 0.18, 0.08, 0.07]\n",
    "merchant_center_weights = center_weights  # reuse same city bias\n",
    "\n",
    "# spread per category (higher for specialty stores)\n",
    "sigma_by_cat = {\n",
    "    'grocery': 0.02, 'gas': 0.02, 'restaurant': 0.03, 'retail': 0.04,\n",
    "    'electronics': 0.06, 'jewelry': 0.06, 'luxury_goods': 0.08\n",
    "}\n",
    "\n",
    "for i in range(1, N_MERCHANTS + 1):\n",
    "    mid = f'MERCHANT_{i:04d}'\n",
    "    cat = np.random.choice(MERCHANT_CATEGORIES, p=category_base_probs)\n",
    "    idx = np.random.choice(len(URBAN_CENTERS), p=merchant_center_weights)\n",
    "    base_lat, base_lon = URBAN_CENTERS[idx][0], URBAN_CENTERS[idx][1]\n",
    "    jitter = np.random.normal(scale=sigma_by_cat[cat])\n",
    "    mlat = base_lat + (np.random.normal(scale=sigma_by_cat[cat]))\n",
    "    mlon = base_lon + (np.random.normal(scale=sigma_by_cat[cat]))\n",
    "    merchants.append((mid, cat, float(round(mlat,6)), float(round(mlon,6))))\n",
    "\n",
    "merch_df = pd.DataFrame(merchants, columns=['merchant_id', 'merchant_category', 'merchant_lat', 'merchant_long'])\n",
    "\n",
    "# Select a small set of collusive merchants (for merchant collusion scenario)\n",
    "NUM_COLLUSIVE_MERCHANTS = 10\n",
    "collusive_merchants = merch_df.sample(n=NUM_COLLUSIVE_MERCHANTS, random_state=SEED).merchant_id.tolist()\n",
    "\n",
    "print('Created', len(cust_df), 'customers and', len(merch_df), 'merchants')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8645607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated baseline transactions: (100000, 18)\n"
     ]
    }
   ],
   "source": [
    "# 3) Generate baseline transactions (legitimate behavior)\n",
    "\n",
    "# precompute total seconds in period for uniform sampling\n",
    "total_seconds = int((END_DATE - START_DATE).total_seconds())\n",
    "\n",
    "# hour seasonality (lunch + evening peaks)\n",
    "hours = np.arange(24)\n",
    "hour_probs = np.zeros(24)\n",
    "# lunch peak (12-14)\n",
    "hour_probs[12:15] += 0.18 / 3\n",
    "# evening peak (18-21)\n",
    "hour_probs[18:22] += 0.40 / 4\n",
    "# morning small peak (8-10)\n",
    "hour_probs[8:11] += 0.12 / 3\n",
    "# spread remaining\n",
    "hour_probs += (1.0 - hour_probs.sum()) / 24\n",
    "hour_probs /= hour_probs.sum()\n",
    "\n",
    "# day-of-week probabilities (weekend slightly busier for leisure)\n",
    "dow_probs = np.array([0.12,0.14,0.14,0.14,0.14,0.16,0.16])\n",
    "dow_probs /= dow_probs.sum()\n",
    "\n",
    "# amount log-normal parameters per category (mu, sigma) on log scale\n",
    "amt_params = {\n",
    "    'grocery': (np.log(500), 0.6),\n",
    "    'gas': (np.log(2000), 0.4),\n",
    "    'restaurant': (np.log(1200), 0.7),\n",
    "    'retail': (np.log(2500), 0.9),\n",
    "    'electronics': (np.log(15000), 1.1),\n",
    "    'jewelry': (np.log(35000), 1.2),\n",
    "    'luxury_goods': (np.log(40000), 1.3)\n",
    "}\n",
    "\n",
    "# sample customers proportional to base_rate (high spenders more often)\n",
    "cust_weights = cust_df['base_rate'].values\n",
    "cust_weights = cust_weights / cust_weights.sum()\n",
    "sampled_customer_indices = np.random.choice(np.arange(N_CUSTOMERS), size=N_ROWS, p=cust_weights)\n",
    "sampled_customers = cust_df.iloc[sampled_customer_indices].reset_index(drop=True)\n",
    "\n",
    "# quick mapping for card numbers (one primary card per customer)\n",
    "card_numbers = [f'CARD_{100000 + i}' for i in range(1, N_CUSTOMERS+1)]\n",
    "cust_card_map = dict(zip(cust_df.customer_id, card_numbers))\n",
    "\n",
    "txn_rows = []\n",
    "for i in range(N_ROWS):\n",
    "    cust = sampled_customers.iloc[i]\n",
    "    cid = cust['customer_id']\n",
    "    # choose merchant category according to customer's preference\n",
    "    cp = np.array(cust['cat_probs'])\n",
    "    cp = cp / cp.sum()\n",
    "    mcat = np.random.choice(MERCHANT_CATEGORIES, p=cp)\n",
    "    # select a merchant id from that category (uniform among same-category merchants)\n",
    "    mlist = merch_df[merch_df['merchant_category'] == mcat]['merchant_id'].tolist()\n",
    "    if not mlist:\n",
    "        merch_choice = merch_df.sample(n=1, random_state=SEED).merchant_id.values[0]\n",
    "    else:\n",
    "        merch_choice = random.choice(mlist)\n",
    "    merch_info = merch_df[merch_df['merchant_id'] == merch_choice].iloc[0]\n",
    "    mlat, mlon = merch_info['merchant_lat'], merch_info['merchant_long']\n",
    "\n",
    "    # timestamp: pick a second in the period, then re-sample hour to respect hour_probs\n",
    "    rand_seconds = random.randint(0, total_seconds)\n",
    "    ts = START_DATE + timedelta(seconds=rand_seconds)\n",
    "    sel_hour = np.random.choice(hours, p=hour_probs)\n",
    "    ts = ts.replace(hour=int(sel_hour), minute=int(np.random.randint(0,60)), second=int(np.random.randint(0,60)))\n",
    "\n",
    "    # amount: log-normal per category\n",
    "    mu, sigma = amt_params[mcat]\n",
    "    amount = float(np.exp(np.random.normal(mu, sigma)))\n",
    "    amount = round(max(1.0, amount), 2)\n",
    "\n",
    "    # compute distance from home to merchant\n",
    "    distance_km = round(haversine(cust['home_lat'], cust['home_long'], mlat, mlon), 2)\n",
    "\n",
    "    txn_rows.append({\n",
    "        'customer_id': cid,\n",
    "        'card_number': cust_card_map[cid],\n",
    "        'timestamp': ts.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "        'amount': amount,\n",
    "        'merchant_id': merch_choice,\n",
    "        'merchant_category': mcat,\n",
    "        'merchant_lat': float(round(mlat,6)),\n",
    "        'merchant_long': float(round(mlon,6)),\n",
    "        'home_lat': float(round(cust['home_lat'],6)),\n",
    "        'home_long': float(round(cust['home_long'],6)),\n",
    "        'distance_from_home': distance_km\n",
    "    })\n",
    "\n",
    "base_df = pd.DataFrame(txn_rows)\n",
    "# add transaction id and default fraud columns\n",
    "base_df.insert(0, 'transaction_id', [f'TXN_{i:08d}' for i in range(1, len(base_df)+1)])\n",
    "base_df['is_fraud'] = 0\n",
    "base_df['fraud_type'] = 'none'\n",
    "# convenience parsed ts\n",
    "base_df['ts_dt'] = pd.to_datetime(base_df['timestamp'])\n",
    "base_df['hour'] = base_df['ts_dt'].dt.hour\n",
    "base_df['day_of_week'] = base_df['ts_dt'].dt.weekday\n",
    "base_df['month'] = base_df['ts_dt'].dt.month\n",
    "\n",
    "print('Generated baseline transactions:', base_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c108e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target fraud count: 2000\n",
      "Injecting merchant collusion...\n",
      "Injecting card cloning...\n",
      "Injecting account takeover events...\n",
      "Injecting velocity probes...\n",
      "Fraud injection completed — current frauds: 1656\n"
     ]
    }
   ],
   "source": [
    "# 4) Fraud pattern injections — refactored into functions for readability\n",
    "\n",
    "def inject_merchant_collusion(df, merch_df, collusive_merchants, n_marked):\n",
    "    \"\"\"Force a subset of mule customers to transact at collusive merchants and mark some as fraud.\"\"\"\n",
    "    # pick mule customers (small set)\n",
    "    mule_customers = df['customer_id'].drop_duplicates().sample(n=50, random_state=SEED).tolist()\n",
    "    coll_txn_indices = df[df['customer_id'].isin(mule_customers)].sample(n=min(700, len(df)), random_state=SEED).index.tolist()\n",
    "\n",
    "    for idx in coll_txn_indices:\n",
    "        m = random.choice(collusive_merchants)\n",
    "        df.at[idx, 'merchant_id'] = m\n",
    "        coll_cat = random.choice(['jewelry', 'electronics', 'luxury_goods'])\n",
    "        df.at[idx, 'merchant_category'] = coll_cat\n",
    "        merch_row = merch_df[merch_df['merchant_id'] == m].iloc[0]\n",
    "        df.at[idx, 'merchant_lat'] = merch_row['merchant_lat']\n",
    "        df.at[idx, 'merchant_long'] = merch_row['merchant_long']\n",
    "        # structured amounts around ₹49k or round numbers\n",
    "        df.at[idx, 'amount'] = float(random.choice([49000, 49500, 49999, 5000, 10000, 25000]))\n",
    "        df.at[idx, 'distance_from_home'] = round(haversine(df.at[idx,'home_lat'], df.at[idx,'home_long'], df.at[idx,'merchant_lat'], df.at[idx,'merchant_long']),2)\n",
    "\n",
    "    # mark first n_marked of those coll_txn_indices as fraud\n",
    "    for idx in coll_txn_indices[:n_marked]:\n",
    "        df.at[idx, 'is_fraud'] = 1\n",
    "        df.at[idx, 'fraud_type'] = 'merchant_collusion'\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_card_cloning(df, merch_df, n_to_inject):\n",
    "    \"\"\"Simulate impossible travel and concurrent sessions for some cards.\"\"\"\n",
    "    cloning_candidates = df.sample(n=2000, random_state=SEED+1).index.tolist()\n",
    "    injected = 0\n",
    "    for idx in cloning_candidates:\n",
    "        if injected >= n_to_inject:\n",
    "            break\n",
    "        row = df.loc[idx]\n",
    "        orig_ts = pd.to_datetime(row['timestamp'])\n",
    "        lat1, lon1 = row['merchant_lat'], row['merchant_long']\n",
    "        # find far merchants (>800 km)\n",
    "        far_merch = merch_df.copy()\n",
    "        far_merch['dist_from_t1'] = far_merch.apply(lambda r: haversine(lat1, lon1, r['merchant_lat'], r['merchant_long']), axis=1)\n",
    "        far_candidates = far_merch[far_merch['dist_from_t1'] > 800]\n",
    "        if far_candidates.empty:\n",
    "            continue\n",
    "        target = far_candidates.sample(n=1, random_state=SEED+injected).iloc[0]\n",
    "        # create/modify another transaction to simulate same card used far away shortly after\n",
    "        delta_minutes = random.randint(5, 120)\n",
    "        new_ts = orig_ts + timedelta(minutes=delta_minutes)\n",
    "        # pick an index to overwrite (simulates an additional transaction)\n",
    "        other_idx = df.sample(n=1, random_state=SEED+100+injected).index[0]\n",
    "        df.at[other_idx, 'customer_id'] = row['customer_id']\n",
    "        df.at[other_idx, 'card_number'] = row['card_number']\n",
    "        df.at[other_idx, 'merchant_id'] = target['merchant_id']\n",
    "        df.at[other_idx, 'merchant_category'] = target['merchant_category']\n",
    "        df.at[other_idx, 'merchant_lat'] = target['merchant_lat']\n",
    "        df.at[other_idx, 'merchant_long'] = target['merchant_long']\n",
    "        df.at[other_idx, 'timestamp'] = new_ts.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        df.at[other_idx, 'ts_dt'] = pd.to_datetime(df.at[other_idx, 'timestamp'])\n",
    "        df.at[other_idx, 'hour'] = df.at[other_idx,'ts_dt'].hour\n",
    "        df.at[other_idx, 'day_of_week'] = df.at[other_idx,'ts_dt'].weekday()\n",
    "        df.at[other_idx, 'month'] = df.at[other_idx,'ts_dt'].month\n",
    "        df.at[other_idx, 'amount'] = round(max(50.0, float(np.exp(np.random.normal(*amt_params[target['merchant_category']])))),2)\n",
    "        df.at[other_idx, 'distance_from_home'] = round(haversine(df.at[other_idx,'home_lat'], df.at[other_idx,'home_long'], df.at[other_idx,'merchant_lat'], df.at[other_idx,'merchant_long']),2)\n",
    "        df.at[other_idx, 'is_fraud'] = 1\n",
    "        df.at[other_idx, 'fraud_type'] = 'card_cloning'\n",
    "        injected += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_account_takeover(df, merch_df, n_to_inject):\n",
    "    \"\"\"Simulate bursts of high-value transactions for some customers (ATO patterns).\"\"\"\n",
    "    ato_customers = df['customer_id'].drop_duplicates().sample(n=int(0.01 * N_CUSTOMERS), random_state=SEED+2).tolist()\n",
    "    injected = 0\n",
    "    for cust in ato_customers:\n",
    "        if injected >= n_to_inject:\n",
    "            break\n",
    "        cust_idxs = df[df['customer_id'] == cust].index.tolist()\n",
    "        if len(cust_idxs) < 3:\n",
    "            continue\n",
    "        k = random.randint(3, 8)\n",
    "        chosen = random.sample(cust_idxs, min(k, len(cust_idxs)))\n",
    "        # select a compromise time\n",
    "        compromise_time = (START_DATE + timedelta(seconds=random.randint(0, total_seconds))).replace(tzinfo=timezone.utc)\n",
    "        base_increase = random.uniform(3.0, 10.0)\n",
    "        for j, idx in enumerate(chosen):\n",
    "            new_ts = compromise_time + timedelta(minutes=j * random.randint(1,10))\n",
    "            df.at[idx, 'timestamp'] = new_ts.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            df.at[idx, 'ts_dt'] = pd.to_datetime(df.at[idx,'timestamp'])\n",
    "            df.at[idx, 'hour'] = df.at[idx,'ts_dt'].hour\n",
    "            df.at[idx, 'day_of_week'] = df.at[idx,'ts_dt'].weekday()\n",
    "            df.at[idx, 'month'] = df.at[idx,'ts_dt'].month\n",
    "            old_amt = df.at[idx,'amount']\n",
    "            new_amt = round(old_amt * base_increase * (1 + np.random.normal(0, 0.2)), 2)\n",
    "            df.at[idx,'amount'] = float(max(100.0, min(new_amt, 150000.0)))\n",
    "            # sometimes switch to high-value categories\n",
    "            if random.random() < 0.7:\n",
    "                new_cat = random.choice(['electronics','jewelry','luxury_goods'])\n",
    "                mlist = merch_df[merch_df['merchant_category'] == new_cat]['merchant_id'].tolist()\n",
    "                if mlist:\n",
    "                    mid = random.choice(mlist)\n",
    "                    merch_row = merch_df[merch_df['merchant_id'] == mid].iloc[0]\n",
    "                    df.at[idx,'merchant_id'] = mid\n",
    "                    df.at[idx,'merchant_category'] = new_cat\n",
    "                    df.at[idx,'merchant_lat'] = merch_row['merchant_lat']\n",
    "                    df.at[idx,'merchant_long'] = merch_row['merchant_long']\n",
    "                    df.at[idx,'distance_from_home'] = round(haversine(df.at[idx,'home_lat'], df.at[idx,'home_long'], df.at[idx,'merchant_lat'], df.at[idx,'merchant_long']),2)\n",
    "            df.at[idx,'is_fraud'] = 1\n",
    "            df.at[idx,'fraud_type'] = 'account_takeover'\n",
    "            injected += 1\n",
    "            if injected >= n_to_inject:\n",
    "                break\n",
    "    return df\n",
    "\n",
    "\n",
    "def inject_velocity_probes(df, max_inject=200):\n",
    "    \"\"\"Small-value rapid-fire probes used to test cards; some portion marked fraud.\"\"\"\n",
    "    probe_cards = df['card_number'].drop_duplicates().sample(n=100, random_state=SEED+3).tolist()\n",
    "    probe_injected = 0\n",
    "    for card in probe_cards:\n",
    "        if probe_injected >= max_inject:\n",
    "            break\n",
    "        idxs = df[df['card_number'] == card].sample(n=min(10, df[df['card_number']==card].shape[0]), random_state=SEED+4).index.tolist()\n",
    "        base_time = (START_DATE + timedelta(seconds=random.randint(0, total_seconds))).replace(tzinfo=timezone.utc)\n",
    "        for j, idx in enumerate(idxs):\n",
    "            new_ts = base_time + timedelta(seconds=j * random.randint(5, 60))\n",
    "            df.at[idx,'timestamp'] = new_ts.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            df.at[idx,'ts_dt'] = pd.to_datetime(df.at[idx,'timestamp'])\n",
    "            df.at[idx,'hour'] = df.at[idx,'ts_dt'].hour\n",
    "            df.at[idx,'day_of_week'] = df.at[idx,'ts_dt'].weekday()\n",
    "            df.at[idx,'month'] = df.at[idx,'ts_dt'].month\n",
    "            df.at[idx,'amount'] = float(round(np.random.choice([10,20,50,99,199,499]) * (1 + np.random.normal(0,0.05)),2))\n",
    "            if random.random() < 0.3 and df.at[idx,'is_fraud'] == 0:\n",
    "                df.at[idx,'is_fraud'] = 1\n",
    "                df.at[idx,'fraud_type'] = 'card_cloning'\n",
    "                probe_injected += 1\n",
    "    return df\n",
    "\n",
    "# Now call the injectors in sequence with tuned counts\n",
    "\n",
    "df = base_df.copy()\n",
    "\n",
    "# Pre-calculate target counts\n",
    "TARGET_FRAUD_COUNT = int(N_ROWS * TARGET_FRAUD_RATE)\n",
    "print('Target fraud count:', TARGET_FRAUD_COUNT)\n",
    "\n",
    "# allocate proportions\n",
    "pct_cloning = 0.4\n",
    "pct_ato = 0.4\n",
    "pct_collusion = 0.2\n",
    "n_cloning = int(TARGET_FRAUD_COUNT * pct_cloning)\n",
    "n_ato = int(TARGET_FRAUD_COUNT * pct_ato)\n",
    "n_collusion = TARGET_FRAUD_COUNT - n_cloning - n_ato\n",
    "\n",
    "# inject\n",
    "print('Injecting merchant collusion...')\n",
    "df = inject_merchant_collusion(df, merch_df, collusive_merchants, n_marked=n_collusion)\n",
    "print('Injecting card cloning...')\n",
    "df = inject_card_cloning(df, merch_df, n_to_inject=n_cloning)\n",
    "print('Injecting account takeover events...')\n",
    "df = inject_account_takeover(df, merch_df, n_to_inject=n_ato)\n",
    "print('Injecting velocity probes...')\n",
    "df = inject_velocity_probes(df, max_inject=200)\n",
    "\n",
    "# Some legitimate holiday spikes (not fraud) to reduce false positives\n",
    "holiday_customers = df['customer_id'].drop_duplicates().sample(n=200, random_state=SEED+5).tolist()\n",
    "for cust in holiday_customers:\n",
    "    idxs = df[df['customer_id'] == cust].sample(n=min(3, df[df['customer_id']==cust].shape[0]), random_state=SEED+6).index.tolist()\n",
    "    for idx in idxs:\n",
    "        df.at[idx,'amount'] = round(df.at[idx,'amount'] * random.uniform(2.0, 5.0),2)\n",
    "        df.at[idx,'is_fraud'] = 0\n",
    "        df.at[idx,'fraud_type'] = 'none'\n",
    "\n",
    "print('Fraud injection completed — current frauds:', int(df['is_fraud'].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d3a0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative fraud count after risk-sampling: 38505\n",
      "Adjusting to hit exact target fraud count: 2000\n",
      "Final fraud count: 2000 target: 2000\n",
      "Sanity OK — rows: 100000\n",
      "Fraud fraction: 0.02\n",
      "Wrote CSV to ./transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# 5) Risk-score post-processing + ensure global fraud rate ~ TARGET_FRAUD_RATE\n",
    "\n",
    "# compute initial simple risk signals\n",
    "# These are short, human-friendly heuristics used to bias final labeling\n",
    "\n",
    "def compute_risk_score_row(r):\n",
    "    score = 0.0\n",
    "    # large distance\n",
    "    if r['distance_from_home'] > 200:\n",
    "        score += 2.0\n",
    "    # very large amounts\n",
    "    if r['amount'] > 50000:\n",
    "        score += 1.5\n",
    "    # odd hours (night)\n",
    "    if int(r['hour']) in range(0,6):\n",
    "        score += 0.8\n",
    "    # flagged as one of the injected frauds already\n",
    "    if r['fraud_type'] in ['merchant_collusion','card_cloning','account_takeover']:\n",
    "        score += 2.5\n",
    "    # round-number structuring\n",
    "    if int(r['amount']) % 1000 == 0:\n",
    "        score += 0.6\n",
    "    return score\n",
    "\n",
    "# compute risk_score for all rows\n",
    "risk_scores = df.apply(compute_risk_score_row, axis=1)\n",
    "# convert to a probability with a logistic transform; threshold will be tuned\n",
    "threshold = 2.5\n",
    "p_fraud = 1 / (1 + np.exp(-(risk_scores - threshold)))\n",
    "\n",
    "# sample probabilistically by these p_fraud values but keep previously injected frauds as high-probability\n",
    "rng = np.random.RandomState(SEED+10)\n",
    "candidate_flags = (rng.rand(len(df)) < p_fraud).astype(int)\n",
    "# combine with existing flags to form a tentative is_fraud\n",
    "# we keep explicit injected frauds where df['is_fraud'] == 1\n",
    "final_flags = ((df['is_fraud'] == 1) | (candidate_flags == 1)).astype(int)\n",
    "\n",
    "current_count = int(final_flags.sum())\n",
    "print('Tentative fraud count after risk-sampling:', current_count)\n",
    "\n",
    "# If we overshot / undershot, adjust by ranking risk_scores\n",
    "TARGET_COUNT = int(TARGET_FRAUD_RATE * len(df))\n",
    "if current_count != TARGET_COUNT:\n",
    "    print('Adjusting to hit exact target fraud count:', TARGET_COUNT)\n",
    "    # Rank rows by risk_scores descending\n",
    "    rank_order = np.argsort(-risk_scores.values)\n",
    "    # build new final_flags all zeros then set top TARGET_COUNT by rank to 1\n",
    "    new_flags = np.zeros(len(df), dtype=int)\n",
    "    top_idxs = rank_order[:TARGET_COUNT]\n",
    "    new_flags[top_idxs] = 1\n",
    "    final_flags = pd.Series(new_flags, index=df.index)\n",
    "\n",
    "# apply final flags and set fraud_type for newly marked cases (if previously none, set to 'synthetic_rule')\n",
    "df['is_fraud'] = final_flags.astype(int)\n",
    "df.loc[(df['is_fraud'] == 1) & (df['fraud_type'] == 'none'), 'fraud_type'] = 'synthetic_rule'\n",
    "\n",
    "print('Final fraud count:', int(df['is_fraud'].sum()), 'target:', TARGET_COUNT)\n",
    "\n",
    "# final housekeeping: recompute distance and timestamp formats\n",
    "from datetime import timezone\n",
    "\n",
    "df['distance_from_home'] = df.apply(lambda r: round(haversine(r['home_lat'], r['home_long'], r['merchant_lat'], r['merchant_long']),2), axis=1)\n",
    "# ensure transaction ids are contiguous\n",
    "df['transaction_id'] = [f'TXN_{i:08d}' for i in range(1, len(df)+1)]\n",
    "# iso timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "final_cols = [\n",
    "    'transaction_id', 'customer_id', 'card_number', 'timestamp', 'amount',\n",
    "    'merchant_id', 'merchant_category', 'merchant_lat', 'merchant_long',\n",
    "    'is_fraud', 'fraud_type', 'hour', 'day_of_week', 'month', 'distance_from_home'\n",
    "]\n",
    "out_df = df[final_cols].copy()\n",
    "\n",
    "# sanity checks\n",
    "assert len(out_df) == N_ROWS\n",
    "print('Sanity OK — rows:', len(out_df))\n",
    "print('Fraud fraction:', out_df['is_fraud'].mean())\n",
    "\n",
    "# write CSV to disk\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "print('Wrote CSV to', OUT_CSV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
